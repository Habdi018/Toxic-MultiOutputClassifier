{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAaPi3JRwaY5"
      },
      "source": [
        "# install the package for transformers \r\n",
        "\r\n",
        "!pip install transformers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM5lkPbM5pyx"
      },
      "source": [
        "# create a folder in which we save our trained model\r\n",
        "!mkdir trained"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJqgycmtCX1"
      },
      "source": [
        "# setting the model name and parameters\r\n",
        "\r\n",
        "MODEL_PARAMS = {'MODEL_NAME': 'distilbert-base-uncased', \r\n",
        "                'N_EPOCHS': 3, \r\n",
        "                'BATCH_SIZE': 32, \r\n",
        "                'MAX_LENGTH': 128, \r\n",
        "                'RDN_NUM': 123, \r\n",
        "                'LEARNING_RATE': 2e-5, \r\n",
        "                'NUM_LABELS': 6}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9b_HbLKsxQv"
      },
      "source": [
        "# building tokenizer and model\r\n",
        "from transformers import AdamW, BertTokenizer, BertForSequenceClassification\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PARAMS['MODEL_NAME'], do_lower_case=True)\r\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_PARAMS['MODEL_NAME'], \r\n",
        "                                                      num_labels=MODEL_PARAMS['NUM_LABELS'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSt11R7y73Wt"
      },
      "source": [
        "# send our model to the GPU device\r\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1FMGpu8UKo_"
      },
      "source": [
        "# computing max sequence length to have an overview of the dataset when tokenized by our models\r\n",
        "\r\n",
        "import pandas\r\n",
        "\r\n",
        "df = pandas.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")\r\n",
        "df[\"len_Tokenized\"] = df[\"comment_text\"].apply(tokenizer.tokenize).apply(len)\r\n",
        "computed_max_sequence_length = df[\"len_Tokenized\"].max()\r\n",
        "\r\n",
        "print(\"The computed max sequence length is: \", computed_max_sequence_length)\r\n",
        "\r\n",
        "# remove added column\r\n",
        "df.drop('len_Tokenized',\r\n",
        "        axis='columns', \r\n",
        "        inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPKokqVtp_Ye"
      },
      "source": [
        "# preparing data for the training stage\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        " \r\n",
        "\r\n",
        "label_names = list(df.columns[2:])\r\n",
        "labels = df[label_names].values\r\n",
        "comments = df.comment_text.values\r\n",
        "\r\n",
        "# tokenization plus padding\r\n",
        "encodings = tokenizer.batch_encode_plus(comments, \r\n",
        "                                        max_length=MODEL_PARAMS['MAX_LENGTH'], \r\n",
        "                                        truncation=True, \r\n",
        "                                        padding=True)\r\n",
        "\r\n",
        "train_inputs, val_inputs, \\\r\n",
        "train_labels, val_labels, \\\r\n",
        "train_masks, val_masks = train_test_split(encodings['input_ids'], \r\n",
        "                                          labels, \r\n",
        "                                          encodings['attention_mask'],\r\n",
        "                                          random_state=MODEL_PARAMS['RDN_NUM'], \r\n",
        "                                          test_size=0.2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNeoE73usGb3"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "# preparing train data\r\n",
        "train_inputs = torch.tensor(train_inputs)\r\n",
        "train_labels = torch.tensor(train_labels)\r\n",
        "train_masks = torch.tensor(train_masks)\r\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)  # Creates a TensorDataset from a vector of tensors.\r\n",
        "train_sampler = RandomSampler(train_data)  # A Sampler that returns random indices.\r\n",
        "\r\n",
        "# preparing validation data\r\n",
        "val_inputs = torch.tensor(val_inputs)\r\n",
        "val_labels = torch.tensor(val_labels)\r\n",
        "val_masks = torch.tensor(val_masks)\r\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)  # Creates a TensorDataset from a vector of tensors.\r\n",
        "val_sampler = SequentialSampler(val_data)  # A Sampler that returns indices sequentially.\r\n",
        "\r\n",
        "# a Python iterable over a dataset\r\n",
        "train_dataloader = DataLoader(train_data, \r\n",
        "                              sampler=train_sampler, \r\n",
        "                              batch_size=MODEL_PARAMS['BATCH_SIZE'])\r\n",
        "val_dataloader = DataLoader(val_data, \r\n",
        "                            sampler=val_sampler, \r\n",
        "                            batch_size=MODEL_PARAMS['BATCH_SIZE'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfj2Z1VLxVYJ"
      },
      "source": [
        "# setting the optimization parameters\r\n",
        "\r\n",
        "param_optimizer = list(model.named_parameters())\r\n",
        "no_decay = ['bias', 'gamma', 'beta']\r\n",
        "\r\n",
        "optimizer_grouped_parameters = [\r\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\r\n",
        "      'weight_decay_rate': 0.01},\r\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\r\n",
        "      'weight_decay_rate': 0.0}\r\n",
        "]\r\n",
        "\r\n",
        "optimizer = AdamW(optimizer_grouped_parameters, \r\n",
        "                  lr=MODEL_PARAMS['LEARNING_RATE'], \r\n",
        "                  correct_bias=True)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a0mQ37-xpLX"
      },
      "source": [
        "\"\"\"\r\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\r\n",
        "\r\n",
        "This loss combines a Sigmoid layer and the BCELoss (Binary Cross Entropy Loss) in one single class. \r\n",
        "\"\"\"\r\n",
        "\r\n",
        "from torch.nn import BCEWithLogitsLoss\r\n",
        "loss_func = BCEWithLogitsLoss()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tICr-mC0Xkp"
      },
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "def evaluate(val_dataloader):\r\n",
        "  \r\n",
        "  model.eval()  #  turn off some specific parts of the model (belong to training and inference phases)\r\n",
        "\r\n",
        "  true_labels, pred_labels = list(), list()\r\n",
        "  for i, batch in enumerate(val_dataloader):\r\n",
        "      batch = tuple(t.to(DEVICE) for t in batch)\r\n",
        "      input_ids, input_mask, labels = batch\r\n",
        "\r\n",
        "      with torch.no_grad():  # Disabling gradient calculation.\r\n",
        "          outs = model(input_ids, token_type_ids=None, attention_mask=input_mask)\r\n",
        "          pred_label = torch.round(torch.sigmoid(outs[0]))\r\n",
        "\r\n",
        "      true_labels.append(labels.to('cpu').numpy())\r\n",
        "      pred_labels.append(pred_label.to('cpu').numpy())\r\n",
        "\r\n",
        "  pred_labels = [pl for pt in pred_labels for pl in pt]  # flatten prediction tensors\r\n",
        "  true_labels = [tl for tt in true_labels for tl in tt]  # flatten true tensors\r\n",
        "  \r\n",
        "  print('Macro F1-score: ', f1_score(true_labels, pred_labels, average='macro'))\r\n",
        "\r\n",
        "  return true_labels, pred_labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3oIrxMEyhVB"
      },
      "source": [
        "from tqdm import trange\r\n",
        "from torch import cuda\r\n",
        "\r\n",
        "DEVICE = 'cuda' if cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "train_loss_set = []\r\n",
        "for _ in trange(MODEL_PARAMS['N_EPOCHS'], desc=\"Epoch\"):\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  # Train step\r\n",
        "  tr_loss = 0\r\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\r\n",
        "  \r\n",
        "  for step, batch in enumerate(train_dataloader):\r\n",
        "    batch = tuple(t.to(DEVICE) for t in batch)\r\n",
        "    input_ids, input_mask, labels = batch\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # Forward pass for multilabel classification\r\n",
        "    outputs = model(input_ids, \r\n",
        "                    token_type_ids=None, \r\n",
        "                    attention_mask=input_mask)\r\n",
        "    logits = outputs[0]\r\n",
        "    loss = loss_func(\r\n",
        "                     # a manual rescaling weight given to the loss of each batch element (a Tensor of size nbatch)\r\n",
        "                     logits.view(-1, MODEL_PARAMS['NUM_LABELS']), \r\n",
        "                     # a weight of positive examples (a vector with length equal to the number of classes)\r\n",
        "                     labels.type_as(logits).view(-1, MODEL_PARAMS['NUM_LABELS']))\r\n",
        "    train_loss_set.append(loss.item())\r\n",
        "\r\n",
        "    loss.backward() # computes dloss/dx for every x which has requires_grad=True.\r\n",
        "    optimizer.step() # updating parameters while using GPU\r\n",
        "\r\n",
        "    tr_loss += loss.item()\r\n",
        "    nb_tr_examples += input_ids.size(0)\r\n",
        "    nb_tr_steps += 1\r\n",
        "\r\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\r\n",
        "\r\n",
        "  # Evaluation step\r\n",
        "  evaluate(val_dataloader)\r\n",
        "\r\n",
        "print(\"Here you go! Your model is trained.\"\r\n",
        "torch.save(model.state_dict(), 'trained/model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsgALSzjig5J"
      },
      "source": [
        "# load the model, input comment and obtain the classes\r\n",
        "# This cell is stand-alone\r\n",
        "\r\n",
        "import torch\r\n",
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\r\n",
        "\r\n",
        "characteristics = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\r\n",
        "\r\n",
        "MODEL_PARAMS = {'MODEL_NAME': 'distilbert-base-uncased', \r\n",
        "                'MAX_LENGTH': 128, \r\n",
        "                'NUM_LABELS': 6}\r\n",
        "\r\n",
        "new_comment = input(\"Please make a comment on your life!\")  # prompt\r\n",
        "\r\n",
        "with torch.no_grad():  # Disabling gradient calculation.\r\n",
        "  load_model = torch.load('trained/model')\r\n",
        "  trained_tokenizer = BertTokenizer.from_pretrained(MODEL_PARAMS['MODEL_NAME'], \r\n",
        "                                                    state_dict = load_model,\r\n",
        "                                                    do_lower_case=True)\r\n",
        "  trained_model = BertForSequenceClassification.from_pretrained(MODEL_PARAMS['MODEL_NAME'], \r\n",
        "                                                state_dict = load_model,\r\n",
        "                                                num_labels=MODEL_PARAMS['NUM_LABELS'])\r\n",
        "  comment_tokenized = trained_tokenizer.encode_plus(new_comment, \r\n",
        "                                                    max_length=MODEL_PARAMS['MAX_LENGTH'],\r\n",
        "                                                    truncation=True,\r\n",
        "                                                    return_tensors='pt')\r\n",
        "  output = trained_model(comment_tokenized['input_ids'], comment_tokenized['attention_mask'])\r\n",
        "  pred_label = torch.sigmoid(output[0])  # passing logits to the sigmoid function\r\n",
        "  round_values = [round(i) for i in pred_label.tolist()[0]]\r\n",
        "  print(\"Your comment is classified as %s\" % list(zip(characteristics, round_values)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
